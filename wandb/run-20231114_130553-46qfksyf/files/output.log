Cannot initialize model with low cpu memory usage because `accelerate` was not found in the environment. Defaulting to `low_cpu_mem_usage=False`. It is strongly recommended to install `accelerate` for faster and less memory-intense model loading. You can do so with:
```
pip install accelerate
```
.
Cannot initialize model with low cpu memory usage because `accelerate` was not found in the environment. Defaulting to `low_cpu_mem_usage=False`. It is strongly recommended to install `accelerate` for faster and less memory-intense model loading. You can do so with:
```
pip install accelerate
```
.
device : cuda:0
  0%|                                                                                                                                                                                          | 0/25 [00:00<?, ?it/s]C:\Users\pc03\anaconda3\envs\pokemon\lib\site-packages\diffusers\models\attention_processor.py:1746: FutureWarning: `LoRAAttnProcessor` is deprecated and will be removed in version 0.26.0. Make sure use AttnProcessor instead by settingLoRA layers to `self.{to_q,to_k,to_v,to_out[0]}.lora_layer` respectively. This will be done automatically when using `LoraLoaderMixin.load_lora_weights`
  deprecate(
  4%|███████                                                                                                                                                                           | 1/25 [00:43<17:19, 43.29s/it]
Traceback (most recent call last):
  File "main.py", line 21, in <module>
    train(args)
  File "C:\Users\pc03\Documents\GitHub\Pokemon-Generator\training.py", line 91, in train
    loss.backward()
  File "C:\Users\pc03\anaconda3\envs\pokemon\lib\site-packages\torch\_tensor.py", line 487, in backward
    torch.autograd.backward(
  File "C:\Users\pc03\anaconda3\envs\pokemon\lib\site-packages\torch\autograd\__init__.py", line 200, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1024.00 MiB (GPU 0; 12.00 GiB total capacity; 23.14 GiB already allocated; 0 bytes free; 24.51 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF